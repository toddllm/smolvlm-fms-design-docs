- **Patch counts:** With a 512×512 input and patch size 16, the SigLIP vision tower produces a 32×32 grid of 1024 patch embeddings. When the `pixel_shuffle_factor` is 4, these 1024 patches are downsampled to an **8×8 grid** (64 latents) by the connector before being passed to the text backbone.

### SigLIP implementation (FMS & HF)
- **FMS:** The SigLIP vision encoder lives in [`fms/models/siglip_vision.py`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/siglip/configuration_siglip.py#L24). The `SiglipVisionConfig` dataclass sets default parameters matching the [`google/siglip-base-patch16-224`](https://huggingface.co/google/siglip-base-patch16-224) model (`hidden_size=768`, `image_size=224`, `patch_size=16`). See [`siglip_vision.py#L29-L60`](https://github.com/foundation-model-stack/foundation-model-stack/blob/main/fms/models/siglip_vision.py#L29-L60) for the full class definition. The [`SiglipVisionEmbeddings`](https://github.com/huggingface/transformers/blob/16c7afd06f70e1ab7a8bb2e19e33e5473297dc55/src/transformers/models/siglip/modeling_siglip.py#L290) module produces patch embeddings of shape `(B, N, 768)`.
- **HF:** Hugging Face provides the official SigLIP model under [`src/transformers/models/siglip`](https://github.com/huggingface/transformers/tree/16c7afd06f70e1ab7a8bb2e19e33e5473297dc55/src/transformers/models/siglip) (see [`configuration_siglip.py`](https://github.com/huggingface/transformers/blob/16c7afd06f70e1ab7a8bb2e19e33e5473297dc55/src/transformers/models/siglip/configuration_siglip.py) and [`modeling_siglip.py`](https://github.com/huggingface/transformers/blob/16c7afd06f70e1ab7a8bb2e19e33e5473297dc55/src/transformers/models/siglip/modeling_siglip.py)). These files define the same vision configuration and modeling logic used in FMS.

### LlavaNext vision tower (FMS)
- FMS also implements a vision tower for the **LlavaNext** model. In [`llava_next.py#L88-L102)`](https://github.com/foundation-model-stack/foundation-model-stack/blob/c9bc7ee5fc524a31505960d63bd899dd0779feb8/fms/models/llava_next.py#L88-L102), the `LlavaNextConfig` constructs a [`SiglipVisionConfig`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/siglip/configuration_siglip.py#L116) with `hidden_size=1152`, `image_size=384`, and `patch_size=14`, and couples it with a Granite text backbone. This produces more patches per image (since 384/14≈27 patches per side) and uses a multimodal projector to align vision features with the Granite text hidden size.
- Hugging Face does not currently expose a LlavaNext model. However, the LlavaNext vision tower shares the same SigLIP encoder core with different dimensions, so you can refer to the FMS file and the HF SigLIP modules for additional context.

### Connector
The [`Idefics3Connector`](https://github.com/huggingface/transformers/blob/16c7afd06f70e1ab7a8bb2e19e33e5473297dc55/src/transformers/models/idefics3/modeling_idefics3.py#L399-L419) bridges the gap between the vision tower and the language model. It performs a **pixel-unshuffle (space-to-depth)** operation controlled by the `pixel_shuffle_factor` (4 for SmolVLM), reducing a 32×32 grid to 8×8. It then projects each of the 64 visual tokens into the **text hidden size** (576 for SmolVLM) via a linear layer. In multi-patch mode, these 64-token blocks are concatenated for each image patch.

### For reference
- **SmolVLM-256M-Instruct (HF) config:** uses `image_size=512`, `patch_size=16`, `vision_hidden=768`, `text_hidden=576`, and `pixel_shuffle_factor=4`. It also sets `resampler_n_latents=64`, which matches the 8×8 downsampled grid.
- **FMS SigLIP implementation:** see [`siglip_vision.py#L29-L60`](https://github.com/foundation-model-stack/foundation-model-stack/blob/main/fms/models/siglip_vision.py#L29-L60).
- **FMS LlavaNext implementation:** see [`llava_next.py#L88-L102`]([https://github.com/foundation-model-stack/foundation-model-stack/blob/main/fms/models/llava_next.py#L10-L60](https://github.com/foundation-model-stack/foundation-model-stack/blob/c9bc7ee5fc524a31505960d63bd899dd0779feb8/fms/models/llava_next.py#L88-L102)) for the vision settings (`hidden_size=1152`, `image_size=384`, `patch_size=14`).
- **HF vision tower classes:**
  - Idefics3 / SmolVLM: see [`configuration_idefics3.py#L24`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/idefics3/configuration_idefics3.py#L24) and [`modeling_idefics3.py#L50`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/idefics3/modeling_idefics3.py#L50).
  - SigLIP: see [`configuration_siglip.py#L24`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/siglip/configuration_siglip.py#L24) and [`modeling_siglip.py#L50`](https://github.com/huggingface/transformers/blob/17fdaf9b7a8a172474675e1d7fe89c78385fa79b/src/transformers/models/siglip/modeling_siglip.py#L50).
